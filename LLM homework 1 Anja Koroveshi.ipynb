{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "tKqPkDKO4x9b"
      },
      "source": [
        "# HW 01: Tweet Sentiment MLP, Parsing, and WSD\n",
        "\n",
        "### This homework has three tasks:\n",
        "1) Tweet Sentiment Classification with a PyTorch MLP — 7 pts\n",
        "2) Transition-based dependency parser (Arc-Standard) — 5 pts\n",
        "3) Lesk Word Sense Disambiguation (basic + extended + eval) — 8 pts\n",
        "\n",
        "**BONUS:** Solve any task with an LLM — **2 pts**\n",
        "\n",
        "#### Total: 20 points (+2 bonus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSF_fbzv4x9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada0ef86-1399-4be2-c7f4-042a44c17279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# You may run these on Colab.\n",
        "!pip -q install spacy nltk torch scikit-learn\n",
        "!python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "737s2JU44x9d"
      },
      "source": [
        "## Pre-requisite code\n",
        "\n",
        "The code in this section will be used in the tasks. **Do not change these lines.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_d-N22KY4x9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a0f34c-eb9f-497b-88e3-fa40052826a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ spaCy model loaded\n",
            "✓ Loaded 5 sentences\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import spacy, nltk, math, random, itertools, collections\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "# NLTK resources\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"omw-1.4\", quiet=True)\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Load spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"✓ spaCy model loaded\")\n",
        "except Exception as e:\n",
        "    print(\"✗ spaCy model not found.\", e)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "@dataclass\n",
        "class DepSentence:\n",
        "    text: str\n",
        "    tokens: List[str]        # 1..n tokens (index 0 is reserved for ROOT)\n",
        "    pos: List[str]           # UPOS or spaCy coarse POS (len=n+1; pos[0]=\"ROOT\")\n",
        "    heads: List[int]         # heads[i] = head index of token i (0..n), heads[0] = -1\n",
        "    rels: List[str]          # dependency labels (len=n+1; rels[0]=\"root\")\n",
        "\n",
        "def spacy_to_dep(sentence: str) -> DepSentence:\n",
        "    doc = nlp(sentence)\n",
        "    tokens = [\"<ROOT>\"] + [t.text for t in doc]\n",
        "    pos     = [\"ROOT\"] + [t.pos_ for t in doc]\n",
        "    heads   = [-1] + [ (t.head.i + 1) if t.head.i != t.i else 0 for t in doc]   # root's head=0\n",
        "    rels    = [\"root\"] + [ t.dep_ for t in doc ]\n",
        "    return DepSentence(sentence, tokens, pos, heads, rels)\n",
        "\n",
        "def uas_las(pred_heads: List[int], gold_heads: List[int], pred_rels: Optional[List[str]] = None, gold_rels: Optional[List[str]] = None) -> Tuple[float, float]:\n",
        "    \"\"\"Unlabeled and labeled attachment scores excluding ROOT (index 0).\"\"\"\n",
        "    assert len(pred_heads) == len(gold_heads)\n",
        "    n = len(gold_heads) - 1\n",
        "    correct_u, correct_l = 0, 0\n",
        "    for i in range(1, n+1):\n",
        "        if pred_heads[i] == gold_heads[i]:\n",
        "            correct_u += 1\n",
        "            if pred_rels and gold_rels and pred_rels[i] == gold_rels[i]:\n",
        "                correct_l += 1\n",
        "    uas = correct_u / n\n",
        "    las = correct_l / n if pred_rels and gold_rels else float(\"nan\")\n",
        "    return uas, las\n",
        "\n",
        "# A tiny toy corpus (short, mostly projective for Task 1)\n",
        "RAW_SENTENCES = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I saw the man with the telescope.\",\n",
        "    \"Students read books in the library.\",\n",
        "    \"She will finish the report tomorrow.\",\n",
        "    \"Flying planes can be dangerous.\"\n",
        "]\n",
        "CORPUS = [spacy_to_dep(s) for s in RAW_SENTENCES]\n",
        "print(\"✓ Loaded\", len(CORPUS), \"sentences\")\n",
        "\n",
        "def pretty_tree(tokens: List[str], heads: List[int]) -> None:\n",
        "    print(\"TOK\\tHEAD\")\n",
        "    for i, tok in enumerate(tokens[1:], start=1):\n",
        "        print(f\"{i}:{tok}\\t{heads[i]}:{tokens[heads[i]] if heads[i] >= 0 else 'NA'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36DYPSaP4x9e"
      },
      "source": [
        "# TASK 1 (7 pts): Tweet Sentiment Classification with a PyTorch MLP\n",
        "\n",
        "**High-level overview.**  \n",
        "Build a **binary sentiment classifier** for tweets (positive vs. negative) using the classic **NLTK Twitter Samples**\n",
        "dataset (5k positive, 5k negative). You will:\n",
        "(1) preprocess with spaCy (lowercase, lemmatize, keep negations),  \n",
        "(2) use a **provided Binary Bag-of-Words** vectorizer (uni/bi-grams), and  \n",
        "(3) train an **MLP in PyTorch** with validation.  \n",
        "Report **accuracy**, a **confusion matrix**, and show sample predictions.\n",
        "\n",
        "**What you will build.**\n",
        "- A **spaCy-based tokenizer** that removes URLs/mentions, strips `#`, keeps alphabetic tokens and **negations** (“no”, “not”, “nor”, “n’t”), removes stopwords except negations, and lemmatizes.\n",
        "- A **PyTorch pipeline**: Dataset/DataLoader, MLP (Linear→ReLU→Dropout→Linear), Adam optimizer, validation split.\n",
        "- An **evaluation** that prints **accuracy**, **confusion matrix**, and a few random predictions with both **sentiment word** and **label**.\n",
        "\n",
        "---\n",
        "\n",
        "### What is provided\n",
        "- Access to the **NLTK Twitter Samples** corpus.\n",
        "- The global random seed and spaCy pipeline from the pre-requisite section (reused here).\n",
        "- A Binary Bag-of-Words vectorizer **already set up** for you for feature extraction and preparation.\n",
        "\n",
        "---\n",
        "\n",
        "### What you should implement\n",
        "1) `tokenize_tweet()` — the tokenizer/lemmatizer that keeps negations and removes noise.  \n",
        "2) `TweetMLP` (Linear→ReLU→Dropout→Linear) and the training loop with validation.  \n",
        "3) Test-set evaluation (**accuracy**, confusion matrix) and sample predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### Grading (7 pts)\n",
        "- **1.1** Data prep & tokenizer/lemmatizer (spaCy + stopwords + negations) — **2.0 pts**  \n",
        "- **1.2** PyTorch Dataset/DataLoader + MLP with Dropout — **2.0 pts**  \n",
        "- **1.3** Training loop with validation — **2.0 pts**  \n",
        "- **1.4** Evaluation: test accuracy — **1.0 pt**\n",
        "\n",
        "  → Grading for this point is only for verifying that the **final reported test accuracy falls within a reasonable/expected range or value**, and **not for performing the evaluation itself.**\n",
        "\n",
        "> **Tips**  \n",
        "> • Keep negation words (“no”, “not”, “nor”, “n’t”) even if they’re stopwords.  \n",
        "> • Fit the vectorizer on **train only**; use the same vectorizer to transform val/test.  \n",
        "> • Note that it can take a couple of minutes for the vectorizer to transform the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PfswSJvd4x9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5f8755-2a87-404c-995b-796c8da51539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Using device: cpu\n",
            "Step 1: Load tweets & build tokenizer\n",
            "✓ Loaded 10000 tweets: 5000 positive, 5000 negative\n",
            "Step 2: Vectorize with Binary Bag-of-Words (fit on train only)\n",
            "✓ Train shape: (8000, 1580), Val: (1000, 1580), Test: (1000, 1580)\n",
            "Step 3: Build DataLoaders & define MLP\n",
            "✓ Model:\n",
            "TweetMLP(\n",
            "  (fc1): Linear(in_features=1580, out_features=256, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "Step 4: Train with validation\n",
            "[TASK1] Epoch 01 | train loss 0.5660 acc 0.7812 | val loss 0.5081 acc 0.7019\n",
            "[TASK1] Epoch 02 | train loss 0.3785 acc 0.7969 | val loss 0.4522 acc 0.7596\n",
            "[TASK1] Epoch 03 | train loss 0.4393 acc 0.7812 | val loss 0.4471 acc 0.7500\n",
            "[TASK1] Epoch 04 | train loss 0.2666 acc 0.9219 | val loss 0.4386 acc 0.7885\n",
            "[TASK1] Epoch 05 | train loss 0.3103 acc 0.8281 | val loss 0.4501 acc 0.7885\n",
            "[TASK1] Epoch 06 | train loss 0.2782 acc 0.8750 | val loss 0.4597 acc 0.7981\n",
            "[TASK1] Epoch 07 | train loss 0.3456 acc 0.8281 | val loss 0.4473 acc 0.8077\n",
            "[TASK1] Epoch 08 | train loss 0.3480 acc 0.8594 | val loss 0.4462 acc 0.8365\n",
            "[TASK1] Epoch 09 | train loss 0.4623 acc 0.7812 | val loss 0.4631 acc 0.8173\n",
            "[TASK1] Epoch 10 | train loss 0.2282 acc 0.8906 | val loss 0.4676 acc 0.8558\n",
            "Step 5: Test evaluation\n",
            "[TASK1] ✓ Test Accuracy: 0.7210\n",
            "[TASK1] Confusion matrix [rows=true 0/1, cols=pred 0/1]:\n",
            "[[377 123]\n",
            " [156 344]]\n",
            "\n",
            "[TASK1] Sample predictions:\n",
            "- Tweet: '@oohdawg_ Hi liv :))'\n",
            "  → Gold: 1 (pos)  |  Pred: 1 (pos)\n",
            "- Tweet: '@luke_brooks I need a twin :('\n",
            "  → Gold: 0 (neg)  |  Pred: 0 (neg)\n",
            "- Tweet: '\"zayn is coming back on july 26\"\\nsrsly guys? how many times did you say that?? :-(('\n",
            "  → Gold: 0 (neg)  |  Pred: 0 (neg)\n",
            "- Tweet: '@ScreamingJoypad A belated good morning to you, sir :D'\n",
            "  → Gold: 1 (pos)  |  Pred: 1 (pos)\n",
            "- Tweet: \"@NiallOfficial from bad to good. My parents were also proud for being me today it's all bc of you guys. Thank you! X NOTICED ME PLEASE :((\"\n",
            "  → Gold: 0 (neg)  |  Pred: 1 (pos)\n",
            "[TASK1 RESULT_CHECKING_POINT] Test Accuracy: 0.721\n"
          ]
        }
      ],
      "source": [
        "import re, random, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from typing import List, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "\n",
        "# Ensure corpora\n",
        "nltk.download(\"twitter_samples\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "# Seed and device\n",
        "try:\n",
        "    RANDOM_SEED\n",
        "except NameError:\n",
        "    RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✓ Using device: {device}\")\n",
        "\n",
        "# Data loading & preprocessing\n",
        "print(\"Step 1: Load tweets & build tokenizer\")\n",
        "\n",
        "# Labels: pos=1, neg=0\n",
        "pos_texts = twitter_samples.strings(\"positive_tweets.json\")\n",
        "neg_texts = twitter_samples.strings(\"negative_tweets.json\")\n",
        "texts = pos_texts + neg_texts\n",
        "labels = [1]*len(pos_texts) + [0]*len(neg_texts)       # binary index 1 for positive feedback\n",
        "print(f\"✓ Loaded {len(texts)} tweets: {len(pos_texts)} positive, {len(neg_texts)} negative\")\n",
        "\n",
        "# Stopwords (keep negations)\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "NEGATIONS = {\"no\", \"not\", \"nor\", \"n't\"}\n",
        "STOP_TW = STOP - NEGATIONS\n",
        "\n",
        "# Reuse spaCy pipeline if available; otherwise load it\n",
        "try:\n",
        "    nlp\n",
        "except NameError:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize_tweet(text: str, use_lemmas: bool = True) -> List[str]:\n",
        "    \"\"\"\n",
        "    spaCy-based tokenizer for tweets.\n",
        "    - Removes URLs and @mentions\n",
        "    - Strips '#' but keeps the hashtag word\n",
        "    - Keeps alphabetic tokens and explicit negations (\"no\", \"not\", \"nor\", \"n't\")\n",
        "    - Lowercases; optionally lemmatizes\n",
        "    - Removes stopwords except negations\n",
        "    - Filters out single-letter noise (except 'i')\n",
        "    Returns a list of tokens (used by CountVectorizer).\n",
        "    \"\"\"\n",
        "\n",
        "    # Strip URLs and mentions\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "    text = re.sub(r\"@\\w+\", \" \", text)\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    text = re.sub(r\"#\", \" \", text)     # remove just the hashtag\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "      if token.is_alpha or token.text in NEGATIONS:   # keep alphabetic and negation tokens\n",
        "        if use_lemmas is True:\n",
        "          lemma = token.lemma_.lower()        #get lemma of eligible token\n",
        "          if (lemma not in STOP_TW or lemma in NEGATIONS) and (len(lemma) > 1 or lemma == \"i\"):\n",
        "            tokens.append(lemma)      # append lemmas that are negations, words with length > 1 and i\n",
        "\n",
        "    return tokens                # return as list of strings\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "# Vectorization (Binary Bag-of-Words over uni/bi-grams)\n",
        "print(\"Step 2: Vectorize with Binary Bag-of-Words (fit on train only)\")\n",
        "\n",
        "# Stratified Train/Val/Test split: 80/10/10\n",
        "X_train_text, X_temp_text, y_train, y_temp = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=RANDOM_SEED, stratify=labels\n",
        ")\n",
        "X_val_text, X_test_text, y_val, y_test = train_test_split(\n",
        "    X_temp_text, y_temp, test_size=0.5, random_state=RANDOM_SEED, stratify=y_temp\n",
        ")\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    tokenizer=lambda s: tokenize_tweet(s, use_lemmas=True),\n",
        "    token_pattern=None,     # needed when using a custom tokenizer\n",
        "    lowercase=False,        # we already lowercase in the tokenizer\n",
        "    ngram_range=(1, 2),     # unigrams and bigrams\n",
        "    min_df=5,               # ignore tokens that don't appear in at least 5 tweets\n",
        "    binary=True\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train_text)\n",
        "X_val   = vectorizer.transform(X_val_text)\n",
        "X_test  = vectorizer.transform(X_test_text)\n",
        "\n",
        "print(f\"✓ Train shape: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# Convert to dense tensors for PyTorch\n",
        "X_train_t = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
        "X_val_t   = torch.tensor(X_val.toarray(),   dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test.toarray(),  dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
        "\n",
        "# PyTorch Dataset/DataLoader + MLP\n",
        "print(\"Step 3: Build DataLoaders & define MLP\")\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "val_ds   = TensorDataset(X_val_t,   y_val_t)\n",
        "test_ds  = TensorDataset(X_test_t,  y_test_t)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
        "\n",
        "class TweetMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden: int = 256, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        out = self.fc1(x)          #first linear\n",
        "        out = self.relu(out)    #relu applied on output\n",
        "        out = self.drop(out)    #dropout applied on output of relu\n",
        "        out = self.fc2(out)   #second linear that will return final output\n",
        "        return out\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "model = TweetMLP(input_dim=X_train_t.shape[1], hidden=256, dropout=0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"✓ Model:\")\n",
        "print(model)\n",
        "\n",
        "# Training with validation\n",
        "print(\"Step 4: Train with validation\")\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "def run_epoch(dl, train: bool = True) -> Tuple[float, float]:\n",
        "\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    for step, (input, output) in enumerate(dl):    # iterate through dataloader input, result pairs\n",
        "\n",
        "      if train == True:    # only dureing training\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      out = model(input)     # get the output from modelling the input\n",
        "      loss = criterion(out, output)   # get loss vlaue between predicted and true result\n",
        "\n",
        "      if train == True:    # only during training\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        _, predicted = torch.max(out, dim=1)\n",
        "        acc = (predicted == output).sum().item()/len(output)    # calculate accuracy\n",
        "\n",
        "    return loss, acc\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
        "    print(f\"[TASK1] Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "print(\"Step 5: Test evaluation\")\n",
        "\n",
        "SENT = {0: \"neg\", 1: \"pos\"}\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_golds = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_golds.extend(yb.numpy().tolist())\n",
        "\n",
        "acc = accuracy_score(all_golds, all_preds)\n",
        "cm  = confusion_matrix(all_golds, all_preds)\n",
        "\n",
        "print(f\"[TASK1] ✓ Test Accuracy: {acc:.4f}\")\n",
        "print(\"[TASK1] Confusion matrix [rows=true 0/1, cols=pred 0/1]:\")\n",
        "print(cm)\n",
        "\n",
        "# Show 5 random predictions\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "idxs = rng.choice(len(X_test_text), size=5, replace=False)\n",
        "print(\"\\n[TASK1] Sample predictions:\")\n",
        "for i in idxs:\n",
        "    gold_lbl = y_test[i]\n",
        "    pred_lbl = all_preds[i]\n",
        "    print(f\"- Tweet: {X_test_text[i]!r}\")\n",
        "    print(f\"  → Gold: {gold_lbl} ({SENT[gold_lbl]})  |  Pred: {pred_lbl} ({SENT[pred_lbl]})\")\n",
        "\n",
        "# RESULT_CHECKING_POINT\n",
        "print(f\"[TASK1 RESULT_CHECKING_POINT] Test Accuracy: {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVVnm1JI4x9f"
      },
      "outputs": [],
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "#\n",
        "# You should see:\n",
        "# • Reasonable val accuracy (~0.70–0.75) after a few epochs.\n",
        "#\n",
        "# Example (illustrative):\n",
        "# [TASK1] Epoch 01 | train loss 0.6044 acc 0.69 | val loss 0.5103 acc 0.7540\n",
        "# ...\n",
        "# [TASK1] Confusion matrix\n",
        "# [[375 125]\n",
        "# [155 345]]\n",
        "#\n",
        "# [TASK1] Sample predictions:\n",
        "# - Tweet: '@oohdawg_ Hi liv :))'\n",
        "#   → Gold: 1 (pos)  |  Pred: 1 (pos)\n",
        "# ...\n",
        "# [TASK1] ✓ Test Accuracy: 0.72 (Expected to be in the range ~0.70–0.75)\n",
        "# ---------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmMTg1Ud4x9g"
      },
      "source": [
        "# TASK 2 (5 pts): Transition-based dependency parsing (Arc-Standard)\n",
        "\n",
        "**High-level overview.**  \n",
        "Dependency parsing links each word to its **head** forming a tree. The **Arc-Standard** system builds this tree **incrementally** with three moves over a `(STACK, BUFFER, ARCS)` configuration. The core constraint is **projectivity**: you only reduce (remove) a token after attaching all of its dependents, which prevents crossing arcs.\n",
        "\n",
        "**What you will build.**  \n",
        "A parser that (1) implements the three transitions, (2) uses a **static oracle** to decide the next legal action from gold heads, and (3) runs a loop that recovers the **gold** projective tree for our toy sentences.\n",
        "\n",
        "---\n",
        "\n",
        "### State & actions\n",
        "- **Initial/terminal states**: start with `STACK=[0]` (ROOT), `BUFFER=[1..n]`, `ARCS=∅`; stop when `BUFFER` is empty and `STACK==[0]`.\n",
        "- **Let** `s1 = stack[-1]` and `s2 = stack[-2]` (when they exist):\n",
        "  - `SHIFT`: move the next token from `BUFFER` to the top of `STACK`.\n",
        "  - `LEFT_ARC`: add `(s1 ← s2)` and remove `s2` (never reduce `ROOT`).\n",
        "  - `RIGHT_ARC`: add `(s2 ← s1)` and remove `s1`.\n",
        "\n",
        "---\n",
        "\n",
        "### What you should implement\n",
        "1) **Transitions** — `SHIFT`, `LEFT_ARC`, `RIGHT_ARC` that mutate `STACK/BUFFER/ARCS` and guard against invalid moves (e.g., empty `BUFFER`, fewer than two items on `STACK`, never reduce `ROOT`).  \n",
        "2) **Static oracle** — Using the gold head array:\n",
        "   - Define **subtree complete**: a node is complete when all its gold dependents are already in `ARCS`.\n",
        "   - Prefer `LEFT_ARC` if `head(s2)=s1` **and** `s2` is complete; else prefer `RIGHT_ARC` if `head(s1)=s2` **and** `s1` is complete; otherwise `SHIFT`.  \n",
        "   - If no action is legal with an empty buffer, raise an error (likely non-projective or an earlier mistake).\n",
        "3) **Parsing loop** — From the initial state, repeatedly query the oracle, apply the action, and on termination convert `ARCS` to a `heads` list with `heads[0] = -1`.\n",
        "\n",
        "---\n",
        "\n",
        "### Grading (5 pts)\n",
        "- 1.1 Transitions (SHIFT/LEFT_ARC/RIGHT_ARC) — **2 pts**  \n",
        "- 1.2 Static oracle (correct preferences & completeness check) — **2 pts**  \n",
        "- 1.3 Exact reconstruction on the toy set (UAS = 1.0 for projective cases) — **1 pt**\n",
        "\n",
        "  → Grading for this point is only for verifying that the **final reported UAS matches the expected correct value (e.g., 1.0 on the toy projective set)**, and **not for performing the evaluation logic itself.**\n",
        "\n",
        "> **Tips**  \n",
        "> • A tiny helper for “subtree complete” keeps the oracle clear.  \n",
        "> • To debug, log `(STACK, BUFFER, action)` for a short sentence.  \n",
        "> • If you get stuck, check that you didn’t reduce a token before attaching its dependents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mI6gx77n4x9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3303f919-c5f6-4cf9-98ac-dcad77fd1a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1, 4, 4, 4, 5, 0, 5, 9, 9, 6, 5]\n",
            "[-1, 4, 4, 4, 5, 0, 5, 9, 9, 6, 5]\n",
            "[TASK2 TEST] The quick brown fox jumps over the lazy dog. -> UAS=1.000\n",
            "TOK\tHEAD\n",
            "1:The\t4:fox\n",
            "2:quick\t4:fox\n",
            "3:brown\t4:fox\n",
            "4:fox\t5:jumps\n",
            "5:jumps\t0:<ROOT>\n",
            "6:over\t5:jumps\n",
            "7:the\t9:dog\n",
            "8:lazy\t9:dog\n",
            "9:dog\t6:over\n",
            "10:.\t5:jumps\n",
            "[-1, 2, 0, 4, 2, 4, 7, 5, 2]\n",
            "[-1, 2, 0, 4, 2, 4, 7, 5, 2]\n",
            "[TASK2 TEST] I saw the man with the telescope. -> UAS=1.000\n",
            "TOK\tHEAD\n",
            "1:I\t2:saw\n",
            "2:saw\t0:<ROOT>\n",
            "3:the\t4:man\n",
            "4:man\t2:saw\n",
            "5:with\t4:man\n",
            "6:the\t7:telescope\n",
            "7:telescope\t5:with\n",
            "8:.\t2:saw\n",
            "[-1, 2, 0, 2, 2, 6, 4, 2]\n",
            "[-1, 2, 0, 2, 2, 6, 4, 2]\n",
            "[TASK2 TEST] Students read books in the library. -> UAS=1.000\n",
            "TOK\tHEAD\n",
            "1:Students\t2:read\n",
            "2:read\t0:<ROOT>\n",
            "3:books\t2:read\n",
            "4:in\t2:read\n",
            "5:the\t6:library\n",
            "6:library\t4:in\n",
            "7:.\t2:read\n",
            "[-1, 3, 3, 0, 5, 3, 3, 3]\n",
            "[-1, 3, 3, 0, 5, 3, 3, 3]\n",
            "[TASK2 TEST] She will finish the report tomorrow. -> UAS=1.000\n",
            "TOK\tHEAD\n",
            "1:She\t3:finish\n",
            "2:will\t3:finish\n",
            "3:finish\t0:<ROOT>\n",
            "4:the\t5:report\n",
            "5:report\t3:finish\n",
            "6:tomorrow\t3:finish\n",
            "7:.\t3:finish\n",
            "[-1, 2, 4, 4, 0, 4, 4]\n",
            "[-1, 2, 4, 4, 0, 4, 4]\n",
            "[TASK2 TEST] Flying planes can be dangerous. -> UAS=1.000\n",
            "TOK\tHEAD\n",
            "1:Flying\t2:planes\n",
            "2:planes\t4:be\n",
            "3:can\t4:be\n",
            "4:be\t0:<ROOT>\n",
            "5:dangerous\t4:be\n",
            "6:.\t4:be\n",
            "[TASK2 RESULT_CHECKING_POINT] Mean UAS = 1.000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ArcStandardParser:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def shift(stack: List[int], buffer: List[int]) -> None:\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        stack.append(buffer[0])      # get token from buffer\n",
        "        buffer.remove(buffer[0])    #remove token from stack\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    @staticmethod\n",
        "    def left_arc(stack: List[int], arcs: Dict[int, int]) -> None:\n",
        "        \"\"\"\n",
        "        Create arc (stack[-1] <- stack[-2]) and remove stack[-2].\n",
        "        I.e., head = stack[-1], dep = stack[-2]\n",
        "        \"\"\"\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        arcs[stack[-2]] = stack[-1]     # last element head of second to last element\n",
        "        stack.remove(stack[-2])\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    @staticmethod\n",
        "    def right_arc(stack: List[int], arcs: Dict[int, int]) -> None:\n",
        "        \"\"\"\n",
        "        Create arc (stack[-2] <- stack[-1]) and remove stack[-1].\n",
        "        I.e., head = stack[-2], dep = stack[-1]\n",
        "        \"\"\"\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "        arcs[stack[-1]] = stack[-2]   # second to last element head of last element\n",
        "        stack.remove(stack[-1])\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    @staticmethod\n",
        "    def oracle_action(stack: List[int], buffer: List[int], gold_heads: List[int], built: Dict[int, int]) -> str:\n",
        "        \"\"\"\n",
        "        Return 'LEFT_ARC' | 'RIGHT_ARC' | 'SHIFT' for a projective gold tree.\n",
        "        Hints:\n",
        "        - You can LEFT_ARC when stack[-2]'s head is stack[-1] and stack[-2] has no remaining right dependents in buffer/stack.\n",
        "        - You can RIGHT_ARC when stack[-1]'s head is stack[-2] and stack[-1] has no remaining dependents in buffer.\n",
        "        - Else SHIFT.\n",
        "        \"\"\"\n",
        "        def subtree_complete(h: int) -> bool:\n",
        "            # all gold dependents of h have already been attached (in `built`)\n",
        "            return all((d in built) for d in range(len(gold_heads)) if gold_heads[d] == h)\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        if len(stack) < 2:   # put at least 2 elements in stack to start comparing\n",
        "          return 'SHIFT'\n",
        "\n",
        "        last_el = stack[-1]     # define elements\n",
        "        sec_last = stack[-2]\n",
        "\n",
        "        if gold_heads[last_el] == sec_last and subtree_complete(last_el):     # if dependent and dependency subtree complete\n",
        "          return 'RIGHT_ARC'\n",
        "        elif gold_heads[sec_last] == last_el and subtree_complete(sec_last):\n",
        "          return 'LEFT_ARC'\n",
        "\n",
        "        return 'SHIFT'\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "    def parse_oracle(self, sent: DepSentence) -> List[int]:\n",
        "        stack = [0]\n",
        "        buffer = list(range(1, len(sent.tokens)))\n",
        "        arcs: Dict[int, int] = {}   # dep -> head\n",
        "\n",
        "        ## YOUR_CODE_STARTS_HERE\n",
        "\n",
        "        gold_heads = sent.heads    # get heads to use in oracle_action\n",
        "\n",
        "        while(buffer or len(stack) > 1):   # loop until buffer is empty or stack has one element\n",
        "\n",
        "          action = self.oracle_action(stack, buffer, gold_heads, arcs)    # decide on action based on gold heads\n",
        "\n",
        "          if action == 'SHIFT':\n",
        "            if buffer:\n",
        "              self.shift(stack, buffer)   #if buffer is not empty shift from buffer to stack\n",
        "            else:     #raise error\n",
        "              raise ValueError(\"No action legal. Likely non-projective or an earlier mistake\")\n",
        "          if action == 'RIGHT_ARC':\n",
        "            self.right_arc(stack, arcs)\n",
        "          if action == 'LEFT_ARC':\n",
        "            self.left_arc(stack, arcs)\n",
        "\n",
        "        heads = [-1] * len(sent.tokens)    # arcs converted to head list\n",
        "        for dep, head in arcs.items():\n",
        "          heads[dep] = head      # create entry (dep, head)\n",
        "\n",
        "        return heads\n",
        "\n",
        "        ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "# TEST_CASES (should be close to UAS=1.0 on projective ones)\n",
        "parser = ArcStandardParser()\n",
        "uas_scores = []\n",
        "for s in CORPUS:\n",
        "    pred = parser.parse_oracle(s)\n",
        "    print(pred)\n",
        "    print(s.heads)\n",
        "\n",
        "    uas, _ = uas_las(pred, s.heads)\n",
        "    uas_scores.append(uas)\n",
        "    print(f\"[TASK2 TEST] {s.text} -> UAS={uas:.3f}\")\n",
        "    pretty_tree(s.tokens, pred)\n",
        "\n",
        "# RESULT_CHECKING_POINT\n",
        "mean_uas = float(np.mean(uas_scores))\n",
        "print(f\"[TASK2 RESULT_CHECKING_POINT] Mean UAS = {mean_uas:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffqMHY2x4x9g"
      },
      "outputs": [],
      "source": [
        "# ------------------------- TESTS & EXPECTED OUTPUT -------------------------\n",
        "#\n",
        "# For the toy corpus, with spaCy 'en_core_web_sm', the oracle parser should\n",
        "# exactly reconstruct the gold trees (projective), giving UAS=1.000 per sentence.\n",
        "#\n",
        "# You should see lines like:\n",
        "#\n",
        "# [TASK2 TEST] The quick brown fox jumps over the lazy dog. -> UAS=1.000\n",
        "# TOK    HEAD\n",
        "# 1:The  <index>:<token>\n",
        "# 2:quick ...\n",
        "# ...  (pretty_tree prints the head index and token for every word)\n",
        "#\n",
        "# [TASK2 TEST] I saw the man with the telescope. -> UAS=1.000\n",
        "# TOK    HEAD\n",
        "# ...\n",
        "# ...\n",
        "# [TASK2 RESULT_CHECKING_POINT] Mean UAS = 1.000\n",
        "# ---------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx77voug4x9g"
      },
      "source": [
        "# TASK 3 (8 pts): Lesk Word Sense Disambiguation (WSD)\n",
        "\n",
        "**High-level overview.**  \n",
        "Word Sense Disambiguation chooses the intended **sense (WordNet synset)** of an ambiguous word from context. The **Lesk** family scores each candidate sense by how many words from its **definition (gloss)** overlap with the **sentence context**. In this task you will build two tightly controlled variants to isolate the impact of **lemmatization**, while keeping everything else fixed and reproducible.\n",
        "\n",
        "**What you will implement.**  \n",
        "Two Lesk scorers that both use the **full sentence** as context and identical preprocessing; they differ only in whether they **lemmatize**:\n",
        "- **Basic Lesk** — surface-form overlap (no lemmatization).  \n",
        "- **Extended Lesk** — same scoring and context, but with **spaCy lemmatization** of both context and gloss.  \n",
        "Evaluate both on the provided gold set and report accuracies.\n",
        "\n",
        "---\n",
        "\n",
        "## Shared setup (for both variants)\n",
        "- **Tokenization**: use **spaCy** to tokenize; keep **alphabetic** tokens and lowercase.\n",
        "- **Stopwords**: use **NLTK** (`nltk.corpus.stopwords`) to remove stopwords.\n",
        "- **Context scope**: **entire sentence**; remove the target token from the context.\n",
        "- **POS filtering**: map spaCy coarse POS → WordNet (`NOUN→wn.NOUN`, `VERB→wn.VERB`, `ADJ→wn.ADJ`, `ADV→wn.ADV`) to prune candidates.\n",
        "- **Scoring & ties**: compare **sets** (unique tokens) via overlap size; break ties by **earliest sense index** (proxy for frequency).\n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 (3 pts) Basic Lesk\n",
        "Build a baseline that:\n",
        "1) Forms the **context set** from the sentence using spaCy surface tokens (no lemmas) → lowercase, drop NLTK stopwords, remove the target.  \n",
        "2) Forms the **sense set** from each candidate synset’s **gloss** with the **same** preprocessing (surface forms).  \n",
        "3) Selects the synset with **maximum set overlap** with the context (tie-break by index).\n",
        "\n",
        "---\n",
        "\n",
        "## 3.2 (3 pts) Extended Lesk (with lemmatization)\n",
        "Repeat 3.1 but **only** change token forms to lemmas:\n",
        "1) **Context lemmas**: spaCy lemmas → lowercase, drop NLTK stopwords; remove the **lemmatized target**.  \n",
        "2) **Gloss lemmas**: spaCy lemmas of the synset **gloss** with the same filtering.  \n",
        "3) Score and tie-break exactly as in Basic.  \n",
        "> The context is still the **full sentence**; any gain comes from **lemmatization**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.3 (2 pts) Evaluation\n",
        "- For each gold item, print: variant (**BASIC/EXT**), sentence, target, predicted synset, gold synset, and `OK=True/False`.  \n",
        "- Report **Accuracy (BASIC)** and **Accuracy (EXT)**.\n",
        "\n",
        "---\n",
        "\n",
        "## Grading (8 pts)\n",
        "- **3.1** Basic Lesk — **3 pts**  \n",
        "- **3.2** Extended Lesk (same pipeline, **spaCy lemmatization** on context & gloss) — **3 pts**  \n",
        "- **3.3** Evaluation (accuracies) — **2 pts**\n",
        "\n",
        "  → Grading for this point is only for verifying that the **final reported accuracies (BASIC and EXT) fall within a reasonable/expected range or value**, and **not for performing the evaluation logic itself.**\n",
        "\n",
        "> **Tips**  \n",
        "> • Centralize preprocessing in helpers and toggle only a “use lemmas” flag to keep the variants identical elsewhere.  \n",
        "> • Remove the target **after** lemmatization in the extended variant.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YRB8MW6U4x9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e72a898-2cfc-41ae-a534-54448e3851ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TASK3 EVAL] BASIC \t sentence=He went to the bank to do few transactions.\t target=bank \tgot=depository_financial_institution.n.01 \tgold=bank.n.09 \taccuracy=0\n",
            "[TASK3 EVAL] BASIC \t sentence=We sat on the bank of the river.\t target=bank \tgot=bank.n.01 \tgold=bank.n.01 \taccuracy=1\n",
            "[TASK3 EVAL] BASIC \t sentence=The musician tuned his bass instrument before the show.\t target=bass \tgot=bass.n.07 \tgold=bass.n.07 \taccuracy=1\n",
            "[TASK3 EVAL] BASIC \t sentence=They caught a large bass in the lakes.\t target=bass \tgot=bass.n.01 \tgold=bass.n.08 \taccuracy=0\n",
            "[TASK3 EVAL] BASIC \t sentence=The chemical plant employs 200 labor workers.\t target=plant \tgot=plant.n.01 \tgold=plant.n.01 \taccuracy=1\n",
            "[TASK3 EVAL] BASIC \t sentence=Please water the plant by the window.\t target=plant \tgot=plant.n.04 \tgold=plant.n.02 \taccuracy=0\n",
            "[TASK3 EVAL] BASIC \t sentence=Place the file record on your desk.\t target=file \tgot=file.n.01 \tgold=file.n.01 \taccuracy=1\n",
            "[TASK3 EVAL] BASIC \t sentence=They will file complaints to the office tomorrow.\t target=file \tgot=file.v.01 \tgold=file.v.01 \taccuracy=1\n",
            "[TASK3 EVAL] BASIC \t sentence=The banks raised the percentage of interest on loans.\t target=interest \tgot=interest.n.04 \tgold=interest.n.04 \taccuracy=1\n",
            "[TASK3 EVAL] BASIC \t sentence=Science fiction is her main interest.\t target=interest \tgot=None \tgold=interest.n.01 \taccuracy=0\n",
            "Avg accuracy (BASIC): 0.600\n",
            "[TASK3 EVAL] EXT \t sentence=He went to the bank to do few transactions.\t target=bank \tgot=bank.n.06 \tgold=bank.n.09 \taccuracy=0\n",
            "[TASK3 EVAL] EXT \t sentence=We sat on the bank of the river.\t target=bank \tgot=bank.n.01 \tgold=bank.n.01 \taccuracy=1\n",
            "[TASK3 EVAL] EXT \t sentence=The musician tuned his bass instrument before the show.\t target=bass \tgot=bass.n.07 \tgold=bass.n.07 \taccuracy=1\n",
            "[TASK3 EVAL] EXT \t sentence=They caught a large bass in the lakes.\t target=bass \tgot=freshwater_bass.n.01 \tgold=bass.n.08 \taccuracy=0\n",
            "[TASK3 EVAL] EXT \t sentence=The chemical plant employs 200 labor workers.\t target=plant \tgot=plant.n.01 \tgold=plant.n.01 \taccuracy=1\n",
            "[TASK3 EVAL] EXT \t sentence=Please water the plant by the window.\t target=plant \tgot=plant.n.04 \tgold=plant.n.02 \taccuracy=0\n",
            "[TASK3 EVAL] EXT \t sentence=Place the file record on your desk.\t target=file \tgot=file.n.01 \tgold=file.n.01 \taccuracy=1\n",
            "[TASK3 EVAL] EXT \t sentence=They will file complaints to the office tomorrow.\t target=file \tgot=file.v.01 \tgold=file.v.01 \taccuracy=1\n",
            "[TASK3 EVAL] EXT \t sentence=The banks raised the percentage of interest on loans.\t target=interest \tgot=interest.n.04 \tgold=interest.n.04 \taccuracy=1\n",
            "[TASK3 EVAL] EXT \t sentence=Science fiction is her main interest.\t target=interest \tgot=None \tgold=interest.n.01 \taccuracy=0\n",
            "Avg accuracy (EXT): 0.600\n",
            "[TASK3 RESULT_CHECKING_POINT] Avg accuracy BASIC vs EXT: 0.600 0.600\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from typing import List, Optional\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Stopwords: NLTK\n",
        "from nltk.corpus import stopwords\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Tokenization helpers: spaCy\n",
        "def spacy_tokens(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize with spaCy and return LOWERCASED surface forms (no lemmas).\n",
        "    - Keep alphabetic tokens only (drop numbers/punct).\n",
        "    - Filter using NLTK stop words.\n",
        "    This is used by *basic* Lesk to avoid lemmatization.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [t.text.lower() for t in doc if t.is_alpha and t.text.lower() not in STOP]\n",
        "\n",
        "def spacy_lemmas(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize + LEMMATIZE with spaCy and return LOWERCASED lemmas.\n",
        "    - Keep alphabetic lemmas only.\n",
        "    - Filter using NLTK stop words.\n",
        "    This is used by *extended* Lesk to show the benefit of lemmatization.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    lemmas = []\n",
        "    for t in doc:\n",
        "        if not t.is_alpha:\n",
        "            continue\n",
        "        lemma = t.lemma_.lower()\n",
        "        if lemma and lemma.isalpha() and lemma not in STOP:\n",
        "            lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def spacy_lemma_of_token(token_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Lemma of a single token string via spaCy (lowercased).\n",
        "    Used to remove the target from the lemmatized context in extended Lesk.\n",
        "    \"\"\"\n",
        "    d = nlp(token_str)\n",
        "    return d[0].lemma_.lower() if len(d) else token_str.lower()\n",
        "\n",
        "# POS mapping\n",
        "def wn_pos_from_coarse(coarse: str):\n",
        "    mappings = {\"NOUN\": wn.NOUN, \"VERB\": wn.VERB, \"ADJ\": wn.ADJ, \"ADV\": wn.ADV}\n",
        "    return mappings.get(coarse, None)\n",
        "\n",
        "# Lesk implementations\n",
        "def lesk_basic(sentence: str, target: str, coarse_pos: Optional[str] = None) -> Optional[wn.synset]:\n",
        "    \"\"\"\n",
        "    Basic Lesk (NO lemmatization):\n",
        "      - spaCy tokenization (surface forms)\n",
        "      - set overlap between sentence tokens and sense gloss\n",
        "      - tie-break by earliest sense index\n",
        "    Returns:\n",
        "      best_synset\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    max_lap = 0\n",
        "    best_synset = None    # initialize for return\n",
        "\n",
        "    context_set = spacy_tokens(sentence)     # tokenize sentence to get context words\n",
        "    if target.lower() in context_set:        # remove target from context set\n",
        "      context_set.remove(target.lower())\n",
        "\n",
        "    pos = wn_pos_from_coarse(coarse_pos)      # If coarse pos is set\n",
        "    if coarse_pos:\n",
        "      sense_set = wn.synsets(target, pos)   # sense set of target word with Parts Of Speech\n",
        "    else:\n",
        "      sense_set = wn.synsets(target)\n",
        "\n",
        "    for sense_t in sense_set:     # for each sense of target\n",
        "      def_targ = sense_t.definition().lower()    #get definition of sense\n",
        "      def_set = spacy_tokens(def_targ)           #get tokens from definition\n",
        "\n",
        "      lap = len(set(context_set) & set(def_set))   # turn to sets and count overlap\n",
        "\n",
        "      for cont in context_set:\n",
        "        sense_cont = wn.synsets(cont)    # get sense of each context\n",
        "        for sense_c in sense_cont:\n",
        "          def_cont = sense_c.definition().lower()    # get definition of each sense\n",
        "          def_cont_set = spacy_tokens(def_cont)     # tokenize the definition\n",
        "\n",
        "          lap += len(set(def_cont_set) & set(def_set))    # count overlap continously\n",
        "                                                        # between definitions of context words and definition of target\n",
        "      if lap > max_lap:\n",
        "        max_lap = lap\n",
        "        best_synset = sense_t  # set max if definition has more overlaping\n",
        "\n",
        "        # First sense that reaches maximum gets returned\n",
        "        # Even if other senses have equal overlap they do not exceed max_lap\n",
        "\n",
        "    return best_synset\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "def lesk_extended(sentence: str,\n",
        "                  target: str,\n",
        "                  coarse_pos: Optional[str] = None) -> Optional[wn.synset]:\n",
        "    \"\"\"\n",
        "    Extended Lesk (WITH lemmatization):\n",
        "      - spaCy tokenization + lemmatization everywhere (context & sense bags)\n",
        "      - set overlap between sentence tokens and sense gloss\n",
        "      - tie-break by earliest sense index\n",
        "    Returns:\n",
        "      best_synset\n",
        "    \"\"\"\n",
        "    ## YOUR_CODE_STARTS_HERE\n",
        "    max_lap = 0\n",
        "    best_synset = None           # same structure as basic lesk\n",
        "\n",
        "    context_set = spacy_lemmas(sentence)         # get sentence lemmas\n",
        "    token_lemma = spacy_lemma_of_token(target)       # lemmatize the target\n",
        "\n",
        "    if token_lemma in context_set:\n",
        "      context_set.remove(token_lemma)    # remove lemma of target from context set\n",
        "\n",
        "    pos = wn_pos_from_coarse(coarse_pos)\n",
        "    if coarse_pos:\n",
        "      sense_set = wn.synsets(target, pos = pos)\n",
        "    else:\n",
        "      sense_set = wn.synsets(target)\n",
        "\n",
        "    for sense_t in sense_set:\n",
        "      def_targ = sense_t.definition().lower()\n",
        "      def_set = spacy_lemmas(def_targ)          # lemmatize + tokenize definition of sense candidate\n",
        "\n",
        "      lap = len(set(context_set) & set(def_set))\n",
        "\n",
        "      for cont in context_set:\n",
        "        sense_cont = wn.synsets(cont)       # get sense for each context word\n",
        "        for sense_c in sense_cont:\n",
        "          def_cont = sense_c.definition().lower()\n",
        "          def_cont_set = spacy_lemmas(def_cont)   #lemmatize + tokenize definition of context word\n",
        "\n",
        "          lap += len(set(def_cont_set) & set(def_set))\n",
        "\n",
        "      if lap > max_lap:\n",
        "        max_lap = lap\n",
        "        best_synset = sense_t\n",
        "\n",
        "    return best_synset\n",
        "    ## YOUR_CODE_ENDS_HERE\n",
        "\n",
        "\n",
        "# Driver that uses FULL sentence for both methods\n",
        "def disambiguate_with_spacy(sent: DepSentence,\n",
        "                            target_index: int,\n",
        "                            extended: bool = True) -> Optional[wn.synset]:\n",
        "    \"\"\"\n",
        "    Resolve the sense of the token at `target_index` in `sent`.\n",
        "    Returns:\n",
        "      (best_synset, best_overlap_count)\n",
        "    \"\"\"\n",
        "    target = sent.tokens[target_index]\n",
        "    coarse_pos = sent.pos[target_index]\n",
        "\n",
        "    if extended:\n",
        "        return lesk_extended(sent.text, target, coarse_pos=coarse_pos)\n",
        "    else:\n",
        "        return lesk_basic(sent.text, target, coarse_pos=coarse_pos)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "def eval_wsd(extended: bool = False) -> float:\n",
        "    correct = 0\n",
        "    for sent, idx, gold in GOLD_WSD:\n",
        "        syn = disambiguate_with_spacy(sent, idx, extended=extended)\n",
        "        got = syn.name() if syn else None\n",
        "        acc = int(got == gold)\n",
        "        correct += int(acc)\n",
        "        print(f\"[TASK3 EVAL] {'EXT' if extended else 'BASIC'} \\t sentence={sent.text}\\t target={sent.tokens[idx]} \"\n",
        "              f\"\\tgot={got} \\tgold={gold} \\taccuracy={acc}\")\n",
        "    n = len(GOLD_WSD)\n",
        "    avg_acc = correct / n if n else 0.0\n",
        "    print(f\"Avg accuracy ({'EXT' if extended else 'BASIC'}): {avg_acc:.3f}\")\n",
        "    return avg_acc\n",
        "\n",
        "# --- Gold set & evaluation ---\n",
        "GOLD_WSD = [\n",
        "    # bank\n",
        "    (spacy_to_dep(\"He went to the bank to do few transactions.\"), 5, \"bank.n.09\"),\n",
        "    (spacy_to_dep(\"We sat on the bank of the river.\"),               5, \"bank.n.01\"),\n",
        "    # bass\n",
        "    (spacy_to_dep(\"The musician tuned his bass instrument before the show.\"),   5, \"bass.n.07\"),\n",
        "    (spacy_to_dep(\"They caught a large bass in the lakes.\"),          5, \"bass.n.08\"),\n",
        "    # plant\n",
        "    (spacy_to_dep(\"The chemical plant employs 200 labor workers.\"),        3, \"plant.n.01\"),\n",
        "    (spacy_to_dep(\"Please water the plant by the window.\"),          4, \"plant.n.02\"),\n",
        "    # file\n",
        "    (spacy_to_dep(\"Place the file record on your desk.\"),                 3, \"file.n.01\"),\n",
        "    (spacy_to_dep(\"They will file complaints to the office tomorrow.\"),           3, \"file.v.01\"),\n",
        "    # interest\n",
        "    (spacy_to_dep(\"The banks raised the percentage of interest on loans.\"),             7, \"interest.n.04\"),\n",
        "    (spacy_to_dep(\"Science fiction is her main interest.\"),          6, \"interest.n.01\"),\n",
        "\n",
        "]\n",
        "\n",
        "# --- TEST_CASES ---\n",
        "acc_basic = eval_wsd(extended=False)\n",
        "acc_ext   = eval_wsd(extended=True)\n",
        "print(\"[TASK3 RESULT_CHECKING_POINT] \"\n",
        "      f\"Avg accuracy BASIC vs EXT: {acc_basic:.3f} {acc_ext:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNEF4wdS4x9h"
      },
      "outputs": [],
      "source": [
        "# -------------------- TESTS & EXPECTED OUTPUT --------------------\n",
        "#\n",
        "# The evaluation prints one line per item, then avg accuracy lines for BASIC and EXT.\n",
        "# Because results can vary slightly with stopword source and WordNet version,\n",
        "# focus on this invariant:\n",
        "#   • Extended (windowed) accuracy should be >= Basic accuracy\n",
        "#   • On this toy set, Extended and Basic accuracies should be 0.600 and 0.700\n",
        "#\n",
        "# Example format (your synset names may vary on some items):\n",
        "# [TASK3 EVAL] BASIC  He deposited cash at the bank near his office.  target=bank  got=bank.n.09  gold=bank.n.09  accuracy=0\n",
        "# ...\n",
        "# Avg accuracy (BASIC): 0.600\n",
        "# [TASK3 EVAL] EXT    He deposited cash at the bank near his office.  target=bank  got=bank.n.09  gold=bank.n.09  accuracy=0\n",
        "# ...\n",
        "# Avg accuracy (EXT): 0.700\n",
        "# [TASK3 RESULT_CHECKING_POINT] Acc basic vs ext: 0.600 0.700\n",
        "# -----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEnJ4Uej4x9h"
      },
      "source": [
        "# BONUS (2 pts): Solve any task with an LLM\n",
        "\n",
        "**Goal.**  \n",
        "Pick **one** of the homework tasks (Task 1, Task 2, or Task 3) and solve it using an **LLM**.  \n",
        "Then **document** what you did and **compare** the LLM’s result to your own pipeline.\n",
        "\n",
        "**What to deliver below.**\n",
        "1) **LLM used** (name + version, e.g., “Llama-3-8B-Instruct”, “GPT-x”, “Claude-x”, “Mistral-x”, etc.).  \n",
        "2) **Prompt(s)** you used.  \n",
        "3) **LLM output** — copy and paste it.  \n",
        "4) **Comparison** to your solution: what matches or differs (quantitative or qualitative).  \n",
        "5) **Reflection**: what the LLM was **good at** vs **bad at**, what it got **right** vs **wrong**.\n",
        "\n",
        "> **No code required.** You do **not** need to run, share, or submit any code used for the LLM generation. Provide only the deliverables listed above.\n",
        "> You may use any LLMs through any interface (API, web UI, local inference).\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXeyWpEJXhRx"
      },
      "source": [
        "# BONUS (2 pts): Solve any task with an LLM\n",
        "\n",
        "**Goal:** Solve Task 2 using a LLM\n",
        "\n",
        "1) **LLM used:** GPT-5\n",
        "\n",
        "2) **Prompt(s):**  \n",
        "\n",
        "    For the following sentences perform transition-based dependency parsing (arc-standard). By implementing three transitions: shift, left arc and right arc over a buffer a stack, store dependencies as arcs. A subtree is complete when all of a node's dependencies are already in arcs. As results produce the projective dependency trees of the sentences.\n",
        "\n",
        "    Explain the steps for one sentence, as only produce outputs for the rest.\n",
        "\n",
        "\n",
        "    1. \"The quick brown fox jumps over the lazy dog.\",\n",
        "\n",
        "    2. \"I saw the man with the telescope.\",\n",
        "\n",
        "    3. \"Students read books in the library.\",\n",
        "\n",
        "    4. \"She will finish the report tomorrow.\",\n",
        "\n",
        "    5. \"Flying planes can be dangerous.\"\n",
        "\n",
        "3) **LLM output**\n",
        "\n",
        "    🔹 Arc-Standard Transition System Recap\n",
        "\n",
        "    Configuration:\n",
        "\n",
        "    Stack (S),\n",
        "    Buffer (B),\n",
        "    Set of arcs (A)\n",
        "\n",
        "    Transitions:\n",
        "\n",
        "    SHIFT: Move the first word from buffer to stack.\n",
        "    ([S], [w_i | B], A) → ([S | w_i], B, A)\n",
        "\n",
        "    LEFT-ARC(l): Add arc (S[1] ← S[0]) and remove S[1] (the head keeps its dependent).\n",
        "    ([S | w_i | w_j], B, A) → ([S | w_j], B, A ∪ {(w_j, l, w_i)})\n",
        "\n",
        "    RIGHT-ARC(l): Add arc (S[1] → S[0]) and remove S[0].\n",
        "    ([S | w_i | w_j], B, A) → ([S | w_i], B, A ∪ {(w_i, l, w_j)})\n",
        "\n",
        "    We stop when the buffer is empty and only the root remains on the stack.\n",
        "\n",
        "    🦊 Sentence 1: “The quick brown fox jumps over the lazy dog.”\n",
        "\n",
        "    Let’s assume “jumps” is the ROOT.\n",
        "\n",
        "\n",
        "    | Step | Stack               | Buffer                                                | Action          | New Arc         |\n",
        "    | ---- | ------------------- | ----------------------------------------------------- | --------------- | --------------- |\n",
        "    | 0    | []                  | [The, quick, brown, fox, jumps, over, the, lazy, dog] | INIT            | —               |\n",
        "    | 1    | [The]               | [quick, brown, fox, jumps, over, the, lazy, dog]      | SHIFT           | —               |\n",
        "    | 2    | [The, quick]        | [brown, fox, jumps, over, the, lazy, dog]             | LEFT-ARC(det)   | (quick ← The)   |\n",
        "    | 3    | [quick]             | [brown, fox, jumps, over, the, lazy, dog]             | SHIFT           | —               |\n",
        "    | 4    | [quick, brown]      | [fox, jumps, over, the, lazy, dog]                    | LEFT-ARC(amod)  | (brown ← quick) |\n",
        "    | 5    | [brown]             | [fox, jumps, over, the, lazy, dog]                    | SHIFT           | —               |\n",
        "    | 6    | [brown, fox]        | [jumps, over, the, lazy, dog]                         | LEFT-ARC(nsubj) | (jumps ← fox)   |\n",
        "    | 7    | [jumps]             | [over, the, lazy, dog]                                | SHIFT           | —               |\n",
        "    | 8    | [jumps, over]       | [the, lazy, dog]                                      | SHIFT           | —               |\n",
        "    | 9    | [jumps, over, the]  | [lazy, dog]                                           | LEFT-ARC(det)   | (the ← over)    |\n",
        "    | 10   | [jumps, over]       | [lazy, dog]                                           | SHIFT           | —               |\n",
        "    | 11   | [jumps, over, lazy] | [dog]                                                 | LEFT-ARC(amod)  | (lazy ← over)   |\n",
        "    | 12   | [jumps, over]       | [dog]                                                 | SHIFT           | —               |\n",
        "    | 13   | [jumps, over, dog]  | []                                                    | LEFT-ARC(pobj)  | (over ← dog)    |\n",
        "    | 14   | [jumps, over]       | []                                                    | RIGHT-ARC(prep) | (jumps → over)  |\n",
        "    | 15   | [jumps]             | []                                                    | —               | root complete ✅ |\n",
        "\n",
        "\n",
        "    Final Arcs (Dependency Tree):\n",
        "\n",
        "    jumps (ROOT)\n",
        "    ├── fox (nsubj)\n",
        "    │     ├── quick (amod)\n",
        "    │     │     └── The (det)\n",
        "    │     └── brown (amod)\n",
        "    └── over (prep)\n",
        "          └── dog (pobj)\n",
        "                ├── the (det)\n",
        "                └── lazy (amod)\n",
        "\n",
        "    📘 Sentence 2: “I saw the man with the telescope.”\n",
        "\n",
        "    ROOT: saw\n",
        "\n",
        "    Arcs:\n",
        "\n",
        "    saw (ROOT)\n",
        "    ├── I (nsubj)\n",
        "    └── man (dobj)\n",
        "          ├── the (det)\n",
        "          └── with (prep)\n",
        "                └── telescope (pobj)\n",
        "                      └── the (det)\n",
        "\n",
        "    📗 Sentence 3: “Students read books in the library.”\n",
        "\n",
        "    ROOT: read\n",
        "\n",
        "    Arcs:\n",
        "\n",
        "    read (ROOT)\n",
        "    ├── Students (nsubj)\n",
        "    └── books (dobj)\n",
        "          ├── in (prep)\n",
        "          │     └── library (pobj)\n",
        "          │           └── the (det)\n",
        "\n",
        "    📙 Sentence 4: “She will finish the report tomorrow.”\n",
        "\n",
        "    ROOT: finish\n",
        "\n",
        "    Arcs:\n",
        "\n",
        "    finish (ROOT)\n",
        "    ├── She (nsubj)\n",
        "    ├── will (aux)\n",
        "    ├── report (dobj)\n",
        "    │     └── the (det)\n",
        "    └── tomorrow (nmod:tmod)\n",
        "\n",
        "    📒 Sentence 5: “Flying planes can be dangerous.”\n",
        "\n",
        "    ROOT: dangerous\n",
        "\n",
        "    Arcs:\n",
        "\n",
        "    dangerous (ROOT)\n",
        "    └── be (cop)\n",
        "          ├── can (aux)\n",
        "          └── planes (nsubj)\n",
        "                └── Flying (amod)\n",
        "\n",
        "\n",
        "\n",
        "4) **Comparison** to your solution:\n",
        "\n",
        "    The logic used by the LLM is correctly derived by the prompt.\n",
        "\n",
        "    Starting by the results for the first sentence, the LLM correctly puts 'jumps' as root, stores three dependencies at 'fox' and three at 'over' (with a subtree of two dependencies at 'dog')\n",
        "\n",
        "    Sentence 2 and 4 also follow the same logic and produce the same dependency trees. However, differences are shown on sentence 3 and 5.\n",
        "\n",
        "    In \"Students read books in the library\", while our model produces a dependency arc between 'in' and the root 'read', the LLM puts 'books' as head of 'in'.\n",
        "\n",
        "    This means that our model understands the sentance as: *Students read books while being at the library.* while the LLM understands: *Students read books that are in the library.*\n",
        "\n",
        "    A bigger difference is observed at \"Flying planes can be dangerous.\" While our model puts the verb \"be\" as root and \"dangerous\" as dependant on the root, the LLM decides on \"dangerous\" as root and \"be\" as the head of the subtree that depends on root.\n",
        "\n",
        "\n",
        "5) **Reflection**:\n",
        "    Due to the results on the previous section, it can be observed that the LLM correctly follows the model, with minor differences in the dependency decisions.\n",
        "\n",
        "    However, these decisions, cannot be concluded as errors or mistakes. Because the sentences can contain ambiguous phrases, both models are prone to interpreting them differently, but that doesn't mean that the dependency tree comes from syntactical errors as both interpretations are correct.\n",
        "\n",
        "    In the case where dangerous is on the place of the root, it shows that the LLM values core meaning sof the sentences more than formal gramatical rules. The latter are followed by our model putting the verb as the root of the tree.\n",
        "\n",
        "    As a result, even though both models produce correct parsing dependencies, the LLM is more prone to meaning-focused interpretations than model rules. This can be due to its constant experience with reading and understanding human-written texts directly fromt he user.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "bHGJTvIq4x9h"
      },
      "source": [
        "# End of HW 01"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}